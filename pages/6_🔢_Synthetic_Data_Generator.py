# pages/6_ðŸ”¢_Synthetic_Data_Generator.py
import streamlit as st
import pandas as pd
import numpy as np
import re
import json
import random
import math
from datetime import datetime, timedelta
from groq import Groq
from auth import check_session
from typing import Dict, List, Any, Optional, Tuple
from collections import Counter, defaultdict
import hashlib
import itertools

# =============================================================================
# ULTIMATE INTELLIGENT DATA ANALYZER & GENERATOR
# LLM thinks like a data scientist + domain expert
# =============================================================================

class UltimateDataGenerator:
    """LLM does DEEP semantic analysis, Python executes perfectly"""
    
    def __init__(self):
        try:
            self.client = Groq(api_key=st.secrets["GROQ_API_KEY"])
            self.available = True
            self.models = {
                "deep_analysis": "llama-3.3-70b-versatile",
                "large_context": "mixtral-8x7b-32768",
            }
        except:
            self.available = False
            st.warning("LLM not available. Using rule-based fallback.")
    
    def ultimate_analyze_dataset(self, df: pd.DataFrame) -> Dict:
        """
        ULTIMATE ANALYSIS: LLM analyzes data like ChatGPT/DeepSeek would
        """
        if not self.available or df.empty:
            return self._create_smart_blueprint(df)
        
        # Get comprehensive samples
        samples = self._get_comprehensive_samples(df)
        
        # Compute deep statistics
        stats = self._compute_deep_statistics(df)
        
        # Domain detection
        domain_hint = self._detect_domain_hint(df)
        
        prompt = self._build_ultimate_analysis_prompt(df, samples, stats, domain_hint)
        
        try:
            messages = [
                {"role": "system", "content": """You are the ULTIMATE data analysis expert. You analyze ANY dataset with DEEP understanding.

                THINK LIKE CHATGPT/DEEPSEEK:
                1. First, UNDERSTAND what this data represents (medical, e-commerce, financial, etc.)
                2. Analyze EACH column SEMANTICALLY - not just data type, but MEANING
                3. Understand REAL-WORLD LOGIC and relationships
                4. Detect PATTERNS, CONSTRAINTS, and BUSINESS RULES
                5. Create PERFECT generation rules that maintain 100% realism

                CRITICAL: Your analysis must be SO GOOD that synthetic data is indistinguishable from real data."""},
                {"role": "user", "content": prompt}
            ]
            
            response = self.client.chat.completions.create(
                model=self.models["deep_analysis"],
                messages=messages,
                temperature=0.1,
                max_tokens=6000,  # More tokens for deep analysis
                response_format={"type": "json_object"}
            )
            
            result = response.choices[0].message.content
            blueprint = self._parse_ultimate_analysis(result, df)
            
            # Enhance with programmatic insights
            blueprint = self._enhance_with_programmatic_insights(blueprint, df)
            
            return blueprint
            
        except Exception as e:
            st.error(f"Ultimate analysis failed: {str(e)}")
            return self._create_smart_blueprint(df)
    
    def _get_comprehensive_samples(self, df: pd.DataFrame) -> List[Dict]:
        """Get comprehensive samples showing different patterns"""
        samples = []
        
        # Get diverse samples: first, middle, last, and random
        indices = list(range(min(5, len(df))))  # First 5
        
        if len(df) > 10:
            # Middle samples
            mid = len(df) // 2
            indices.extend([mid-1, mid, mid+1])
            
            # Last samples
            indices.extend([-3, -2, -1])
            
            # Random samples for diversity
            random_indices = random.sample(range(5, len(df)-5), min(3, len(df)-10))
            indices.extend(random_indices)
        
        # Remove duplicates and ensure valid indices
        indices = sorted(set(idx for idx in indices if 0 <= idx < len(df)))
        
        for idx in indices[:15]:  # Max 15 comprehensive samples
            row = df.iloc[idx]
            sample = {}
            for col in df.columns:
                val = row[col]
                if pd.isna(val):
                    sample[col] = None
                elif isinstance(val, (int, np.integer)):
                    sample[col] = int(val)
                elif isinstance(val, (float, np.floating)):
                    sample[col] = float(val)
                else:
                    # Keep full string for analysis
                    sample[col] = str(val)
            samples.append(sample)
        
        return samples
    
    def _compute_deep_statistics(self, df: pd.DataFrame) -> Dict:
        """Compute deep statistics for semantic understanding"""
        stats = {}
        
        for col in df.columns:
            non_null = df[col].dropna()
            if len(non_null) == 0:
                stats[col] = {"empty": True}
                continue
            
            col_stats = {
                "null_count": df[col].isnull().sum(),
                "total": len(df[col]),
                "unique_count": len(non_null.unique()),
                "unique_ratio": len(non_null.unique()) / len(non_null),
                "most_common": [],
                "sample_patterns": []
            }
            
            # Most common values
            if len(non_null) > 0:
                value_counts = non_null.value_counts().head(5)
                col_stats["most_common"] = [{"value": str(val), "count": int(cnt)} 
                                           for val, cnt in value_counts.items()]
            
            # Detect patterns in samples
            samples = non_null.head(10).astype(str).tolist()
            patterns = self._detect_semantic_patterns(col, samples)
            col_stats["sample_patterns"] = patterns
            
            # Type-specific stats
            if pd.api.types.is_numeric_dtype(df[col]):
                col_stats.update({
                    "type": "numeric",
                    "min": float(non_null.min()),
                    "max": float(non_null.max()),
                    "mean": float(non_null.mean()),
                    "std": float(non_null.std()),
                    "has_decimals": any(not float(x).is_integer() for x in non_null.head(20) if pd.notna(x))
                })
            else:
                col_stats["type"] = "text"
                # Text analysis
                avg_len = np.mean([len(str(x)) for x in non_null.head(50)])
                col_stats["avg_length"] = float(avg_len)
            
            stats[col] = col_stats
        
        # Cross-column statistics
        stats["relationships"] = self._find_potential_relationships(df)
        
        return stats
    
    def _detect_semantic_patterns(self, col_name: str, samples: List[str]) -> List[str]:
        """Detect semantic patterns in column values"""
        patterns = []
        col_lower = col_name.lower()
        
        # Check for IDs
        if any(x in col_lower for x in ['id', 'no', 'number', 'code', 'ref']):
            if samples and all(re.match(r'^[A-Z]{2}\d{3,}$', s) for s in samples[:3] if s):
                patterns.append("Prefixed sequential ID (e.g., AP001)")
            elif samples and all(re.match(r'^\d+$', s) for s in samples[:3] if s):
                patterns.append("Numeric ID")
        
        # Check for names
        if any(x in col_lower for x in ['name', 'patient', 'doctor', 'customer', 'user']):
            name_samples = [s for s in samples[:5] if s and len(s.split()) >= 2]
            if len(name_samples) >= 3:
                patterns.append("Full names (First Last)")
                # Check cultural patterns
                if any('singh' in s.lower() or 'kumar' in s.lower() or 'patel' in s.lower() for s in name_samples):
                    patterns.append("Indian names detected")
                elif any('smith' in s.lower() or 'johnson' in s.lower() or 'williams' in s.lower() for s in name_samples):
                    patterns.append("Western names detected")
        
        # Check for dates
        date_patterns = [
            (r'\d{2}-\d{2}-\d{4}', 'DD-MM-YYYY'),
            (r'\d{4}-\d{2}-\d{2}', 'YYYY-MM-DD'),
            (r'\d{2}/\d{2}/\d{4}', 'MM/DD/YYYY'),
        ]
        for pattern, format_name in date_patterns:
            if any(re.match(pattern, s) for s in samples if s):
                patterns.append(f"Date format: {format_name}")
                break
        
        # Check for times
        time_patterns = [
            (r'\d{1,2}:\d{2} [AP]M', '12-hour time'),
            (r'\d{2}:\d{2}:\d{2}', '24-hour with seconds'),
            (r'\d{2}:\d{2}', '24-hour time'),
        ]
        for pattern, format_name in time_patterns:
            if any(re.match(pattern, s) for s in samples if s):
                patterns.append(f"Time format: {format_name}")
                break
        
        # Check for phones
        phone_samples = [re.sub(r'\D', '', s) for s in samples[:3] if s]
        if phone_samples and all(len(p) == 10 for p in phone_samples):
            if all(p[0] in '6789' for p in phone_samples):
                patterns.append("Indian mobile numbers (10 digits, starts 6-9)")
            else:
                patterns.append("10-digit phone numbers")
        
        # Check for emails
        if any('@' in s and '.' in s.split('@')[-1] for s in samples[:3] if s):
            patterns.append("Email addresses")
        
        # Check for amounts/money
        if any(x in col_lower for x in ['price', 'fee', 'amount', 'cost', 'salary', 'revenue']):
            money_samples = [s for s in samples[:3] if s and re.match(r'^\d+(\.\d{2})?$', s.replace(',', ''))]
            if money_samples:
                patterns.append("Monetary amounts")
                if all(s.endswith('.00') or s.endswith('.99') for s in money_samples):
                    patterns.append("Common price endings (.00, .99)")
        
        return patterns
    
    def _find_potential_relationships(self, df: pd.DataFrame) -> List[str]:
        """Find potential relationships between columns"""
        relationships = []
        
        # Name-Gender relationships
        name_cols = [col for col in df.columns if any(x in col.lower() for x in ['name', 'patient', 'customer'])]
        gender_cols = [col for col in df.columns if 'gender' in col.lower() or 'sex' in col.lower()]
        
        for name_col in name_cols:
            for gender_col in gender_cols:
                if name_col != gender_col:
                    # Check a few samples
                    for idx in range(min(5, len(df))):
                        name = str(df.iloc[idx][name_col]) if pd.notna(df.iloc[idx][name_col]) else ""
                        gender = str(df.iloc[idx][gender_col]) if pd.notna(df.iloc[idx][gender_col]) else ""
                        
                        if name and gender:
                            name_lower = name.lower()
                            if ('singh' in name_lower or 'kumar' in name_lower) and gender.upper() == 'M':
                                relationships.append(f"{name_col} â†’ {gender_col}: Names with Singh/Kumar are usually Male")
                                break
                            elif ('devi' in name_lower or 'kumari' in name_lower) and gender.upper() == 'F':
                                relationships.append(f"{name_col} â†’ {gender_col}: Names with Devi/Kumari are usually Female")
                                break
        
        # Date relationships
        date_cols = [col for col in df.columns if 'date' in col.lower()]
        if len(date_cols) >= 2:
            relationships.append(f"Date sequence: {date_cols[0]} typically before {date_cols[1]}")
        
        # Amount-category relationships
        amount_cols = [col for col in df.columns if any(x in col.lower() for x in ['amount', 'fee', 'price', 'cost'])]
        category_cols = [col for col in df.columns if any(x in col.lower() for x in ['category', 'type', 'department', 'product'])]
        
        if amount_cols and category_cols:
            relationships.append(f"{category_cols[0]} influences {amount_cols[0]} values")
        
        return relationships
    
    def _detect_domain_hint(self, df: pd.DataFrame) -> str:
        """Detect domain from column names and sample data"""
        column_names = ' '.join(df.columns).lower()
        
        # Medical domain
        medical_keywords = ['patient', 'doctor', 'appointment', 'diagnosis', 'medical', 'hospital', 
                           'clinic', 'treatment', 'prescription', 'surgery']
        if any(keyword in column_names for keyword in medical_keywords):
            return "medical_healthcare"
        
        # E-commerce domain
        ecommerce_keywords = ['order', 'product', 'customer', 'price', 'quantity', 'shipping',
                             'payment', 'cart', 'invoice', 'delivery']
        if any(keyword in column_names for keyword in ecommerce_keywords):
            return "ecommerce_retail"
        
        # Financial domain
        financial_keywords = ['transaction', 'account', 'bank', 'amount', 'balance', 'transfer',
                             'loan', 'credit', 'debit', 'interest']
        if any(keyword in column_names for keyword in financial_keywords):
            return "financial_banking"
        
        # Education domain
        education_keywords = ['student', 'teacher', 'course', 'grade', 'marks', 'attendance',
                             'school', 'college', 'university', 'exam']
        if any(keyword in column_names for keyword in education_keywords):
            return "education_academic"
        
        # Human Resources domain
        hr_keywords = ['employee', 'salary', 'department', 'manager', 'hire', 'position',
                      'performance', 'attendance', 'leave', 'recruitment']
        if any(keyword in column_names for keyword in hr_keywords):
            return "hr_employment"
        
        return "general"
    
    def _build_ultimate_analysis_prompt(self, df: pd.DataFrame, samples: List[Dict], 
                                       stats: Dict, domain_hint: str) -> str:
        """Build ULTIMATE analysis prompt for deep understanding"""
        
        # Column analysis summary
        column_analysis = []
        for col in df.columns:
            col_stat = stats.get(col, {})
            analysis = f"**{col}**: "
            
            if col_stat.get("type") == "numeric":
                analysis += f"Numeric, Range: {col_stat.get('min', '?')}-{col_stat.get('max', '?')}"
                if col_stat.get("has_decimals"):
                    analysis += ", Has decimals"
            else:
                analysis += f"Text, Avg length: {col_stat.get('avg_length', '?')}"
            
            analysis += f", Unique: {col_stat.get('unique_ratio', 0):.1%}"
            
            patterns = col_stat.get("sample_patterns", [])
            if patterns:
                analysis += f", Patterns: {', '.join(patterns[:3])}"
            
            column_analysis.append(analysis)
        
        prompt = f"""
        # ULTIMATE DATASET ANALYSIS
        
        ## DATASET OVERVIEW
        - Total rows: {len(df)}
        - Total columns: {len(df.columns)}
        - Columns: {', '.join(df.columns)}
        - Domain hint: {domain_hint}
        
        ## COLUMN ANALYSIS
        {chr(10).join(column_analysis)}
        
        ## COMPREHENSIVE SAMPLE DATA (showing diversity)
        {json.dumps(samples, indent=2, default=str)}
        
        ## YOUR MISSION: DEEP SEMANTIC ANALYSIS
        
        Analyze this dataset like ChatGPT/DeepSeek would - with DEEP understanding:
        
        ### STEP 1: UNDERSTAND THE DOMAIN & CONTEXT
        1. What is this data REALLY about? (Medical appointments? E-commerce orders? Financial transactions?)
        2. What country/region is this from? (India? USA? Global?)
        3. What industry/domain does this belong to?
        
        ### STEP 2: ANALYZE EACH COLUMN SEMANTICALLY
        For EACH column, answer:
        1. What does this column REPRESENT in the real world?
        2. What SPECIFIC patterns/formatting does it have?
        3. What are realistic values/ranges for this column?
        4. What constraints/rules apply to this column?
        
        ### STEP 3: UNDERSTAND RELATIONSHIPS & BUSINESS RULES
        1. How are columns RELATED to each other?
        2. What business logic/rules exist in this data?
        3. What REAL-WORLD constraints apply?
        4. What would make synthetic data look FAKE? (Avoid these!)
        
        ### STEP 4: CREATE PERFECT GENERATION RULES
        For synthetic data to be INDISTINGUISHABLE from real data:
        1. What exact patterns to follow for each column?
        2. How to maintain relationships between columns?
        3. What value pools/ranges to use?
        4. How to ensure 100% realism?
        
        ## OUTPUT FORMAT
        Return a JSON object with this structure:
        {{
            "dataset_understanding": {{
                "domain": "medical_appointments_india",
                "purpose": "Patient appointment records in Indian hospital",
                "country_region": "India",
                "data_quality_notes": "Notes on data quality issues found"
            }},
            
            "semantic_column_analysis": {{
                "column_name_1": {{
                    "semantic_meaning": "Appointment ID",
                    "real_world_purpose": "Unique identifier for medical appointments",
                    "detected_patterns": ["AP### format", "Sequential numbers"],
                    "format_specification": "AP followed by 3-digit number",
                    "realistic_rules": ["Continue sequence from last ID", "No duplicates", "Fixed format"],
                    "generation_instructions": "Generate AP026, AP027, AP028...",
                    "value_pool_or_range": "N/A - generated sequentially"
                }},
                "column_name_2": {{
                    "semantic_meaning": "Patient full name",
                    "real_world_purpose": "Patient's name for identification",
                    "detected_patterns": ["Indian names", "First Last format", "Cultural naming conventions"],
                    "format_specification": "FirstName LastName (Indian names)",
                    "realistic_rules": ["Indian names only", "Gender indicators in surnames", "Proper capitalization"],
                    "generation_instructions": "Generate realistic Indian names with proper gender alignment",
                    "value_pool_or_range": {{
                        "indian_male_names": ["Rahul", "Amit", "Raj", "Sanjay"],
                        "indian_female_names": ["Priya", "Neha", "Anjali", "Sneha"],
                        "indian_surnames": ["Patel", "Sharma", "Singh", "Kumar", "Gupta"]
                    }}
                }}
            }},
            
            "relationships_and_constraints": [
                {{
                    "type": "name_gender_relationship",
                    "description": "Gender must match name patterns",
                    "rule": "If surname is Singh/Kumar â†’ Gender = M, If surname is Devi/Kumari â†’ Gender = F",
                    "implementation": "Check name for gender indicators before assigning gender"
                }},
                {{
                    "type": "department_diagnosis_link",
                    "description": "Medical diagnosis must match department",
                    "rule": "ENT department should have ear/nose/throat issues, Cardiology for heart issues",
                    "implementation": "Use department-specific diagnosis lists"
                }}
            ],
            
            "generation_strategy": {{
                "overall_approach": "Generate IDs first, then names, then assign gender based on names, then medical details",
                "step_by_step": [
                    "1. Generate sequential appointment IDs",
                    "2. Generate realistic Indian names",
                    "3. Assign gender based on name patterns",
                    "4. Assign age with realistic distribution",
                    "5. Generate Indian phone numbers",
                    "6. Assign department with realistic frequencies",
                    "7. Assign doctor from department",
                    "8. Generate recent dates",
                    "9. Generate appointment times during business hours",
                    "10. Assign department-appropriate diagnosis",
                    "11. Set realistic fee based on department",
                    "12. Set status (mostly Completed)"
                ]
            }},
            
            "quality_requirements": [
                "NO gender-name mismatches (Rahul should not be Female)",
                "NO inappropriate diagnoses (ENT should not have Chest Pain)",
                "NO invalid phone numbers (must be 10 digits Indian mobile)",
                "NO unrealistic ages (18-70 for adults)",
                "NO placeholder values",
                "ALL values must make real-world sense"
            ]
        }}
        
        ## CRITICAL REQUIREMENTS
        1. Be EXTREMELY detailed and thorough
        2. Think about REAL-WORLD logic, not just patterns
        3. Consider cultural/regional context
        4. Identify what would make data look FAKE and avoid it
        5. Create generation rules that ensure 100% realism
        
        Your analysis will be used to generate PERFECT synthetic data that is indistinguishable from real data.
        """
        
        return prompt
    
    def _parse_ultimate_analysis(self, result: str, df: pd.DataFrame) -> Dict:
        """Parse the ultimate analysis result"""
        try:
            data = json.loads(result)
            
            # Ensure all columns are covered
            for col in df.columns:
                if col not in data.get("semantic_column_analysis", {}):
                    data.setdefault("semantic_column_analysis", {})[col] = {
                        "semantic_meaning": "Unknown column",
                        "real_world_purpose": "Unknown purpose",
                        "detected_patterns": ["No patterns detected"],
                        "generation_instructions": "Use values from original data",
                        "value_pool_or_range": df[col].dropna().unique().tolist()[:20]
                    }
            
            # Add metadata
            data["analysis_timestamp"] = datetime.now().isoformat()
            data["original_shape"] = {"rows": len(df), "columns": len(df.columns)}
            data["column_names"] = df.columns.tolist()
            
            return data
            
        except Exception as e:
            st.warning(f"Could not parse ultimate analysis: {str(e)}")
            return self._create_smart_blueprint(df)
    
    def _enhance_with_programmatic_insights(self, blueprint: Dict, df: pd.DataFrame) -> Dict:
        """Enhance blueprint with programmatic insights"""
        
        # Extract patterns from semantic analysis
        semantic_analysis = blueprint.get("semantic_column_analysis", {})
        
        # Create simplified column configs for generation
        column_configs = {}
        
        for col in df.columns:
            sem_config = semantic_analysis.get(col, {})
            col_config = {
                "semantic_meaning": sem_config.get("semantic_meaning", "unknown"),
                "patterns": sem_config.get("detected_patterns", []),
                "generation_rules": [sem_config.get("generation_instructions", "Use realistic values")],
                "value_pool": sem_config.get("value_pool_or_range", [])
            }
            
            # Detect column type from patterns
            col_type = self._infer_column_type_from_semantics(sem_config, df[col])
            col_config["column_type"] = col_type
            
            # Add numeric ranges if applicable
            if pd.api.types.is_numeric_dtype(df[col]):
                non_null = df[col].dropna()
                if len(non_null) > 0:
                    col_config["numeric_range"] = {
                        "min": float(non_null.min()),
                        "max": float(non_null.max())
                    }
            
            column_configs[col] = col_config
        
        # Simplify relationships
        simplified_relationships = []
        for rel in blueprint.get("relationships_and_constraints", []):
            if isinstance(rel, dict):
                simplified_relationships.append(rel.get("rule", rel.get("description", "")))
            else:
                simplified_relationships.append(str(rel))
        
        # Create enhanced blueprint
        enhanced_blueprint = {
            "dataset_context": blueprint.get("dataset_understanding", {}).get("domain", "general"),
            "columns": column_configs,
            "relationships": simplified_relationships,
            "generation_strategy": blueprint.get("generation_strategy", {}).get("step_by_step", []),
            "quality_requirements": blueprint.get("quality_requirements", []),
            "semantic_analysis": semantic_analysis,  # Keep full semantic analysis
            "original_columns": df.columns.tolist(),
            "original_row_count": len(df)
        }
        
        return enhanced_blueprint
    
    def _infer_column_type_from_semantics(self, sem_config: Dict, series: pd.Series) -> str:
        """Infer column type from semantic analysis"""
        patterns = sem_config.get("detected_patterns", [])
        semantic = sem_config.get("semantic_meaning", "").lower()
        
        # Check for IDs
        if any(x in semantic for x in ['id', 'number', 'code', 'reference']):
            return "sequential_id"
        
        # Check for names
        if any(x in semantic for x in ['name', 'patient', 'doctor', 'customer']):
            return "human_name"
        
        # Check for dates/times
        if any(x in semantic for x in ['date', 'time', 'datetime']):
            if 'time' in semantic or any('time' in str(p).lower() for p in patterns):
                return "time"
            return "date"
        
        # Check for phones/emails
        if 'phone' in semantic or any('phone' in str(p).lower() for p in patterns):
            return "phone_number"
        if 'email' in semantic or any('email' in str(p).lower() for p in patterns):
            return "email_address"
        
        # Check for amounts
        if any(x in semantic for x in ['price', 'fee', 'amount', 'cost', 'salary']):
            return "monetary_amount"
        
        # Check patterns
        patterns_str = ' '.join(str(p).lower() for p in patterns)
        if any(x in patterns_str for x in ['indian', 'western', 'name']):
            return "human_name"
        if any(x in patterns_str for x in ['date', 'dd-mm-yyyy', 'yyyy-mm-dd']):
            return "date"
        
        # Default based on data
        return self._detect_basic_column_type(series)
    
    def _detect_basic_column_type(self, series: pd.Series) -> str:
        """Basic column type detection"""
        non_null = series.dropna()
        if len(non_null) == 0:
            return "unknown"
        
        # Numeric detection
        try:
            numeric_count = pd.to_numeric(non_null.head(20), errors='coerce').notna().sum()
            if numeric_count > len(non_null.head(20)) * 0.8:
                if all(float(x).is_integer() for x in non_null.head(10) if pd.notna(x)):
                    return "numeric_integer"
                return "numeric_float"
        except:
            pass
        
        # Categorical detection
        unique_ratio = len(non_null.unique()) / len(non_null)
        if unique_ratio < 0.3 and len(non_null.unique()) <= 20:
            return "categorical_fixed"
        
        return "text_description"
    
    def _create_smart_blueprint(self, df: pd.DataFrame) -> Dict:
        """Create smart blueprint when LLM is not available"""
        # This uses the same logic as before but enhanced
        blueprint = {
            "dataset_context": "general_dataset",
            "columns": {},
            "relationships": [],
            "generation_strategy": ["Generate each column based on observed patterns"],
            "quality_requirements": ["Maintain data types", "Use realistic values"],
            "original_columns": df.columns.tolist(),
            "original_row_count": len(df)
        }
        
        for col in df.columns:
            col_data = df[col].dropna()
            
            # Try to infer semantic meaning from column name
            col_lower = col.lower()
            semantic = "unknown"
            
            if 'id' in col_lower or 'no' in col_lower or 'number' in col_lower:
                semantic = "identifier"
            elif 'name' in col_lower:
                semantic = "person_name"
            elif 'date' in col_lower:
                semantic = "date"
            elif 'time' in col_lower:
                semantic = "time"
            elif 'phone' in col_lower:
                semantic = "phone_number"
            elif 'email' in col_lower:
                semantic = "email_address"
            elif any(x in col_lower for x in ['price', 'fee', 'amount', 'cost']):
                semantic = "monetary_amount"
            
            col_config = {
                "semantic_meaning": semantic,
                "patterns": self._detect_semantic_patterns(col, col_data.head(5).astype(str).tolist()),
                "column_type": self._detect_basic_column_type(df[col]),
                "generation_rules": ["Generate values similar to original patterns"]
            }
            
            if col_config["column_type"] in ["categorical_fixed", "human_name"] and len(col_data) > 0:
                col_config["value_pool"] = col_data.unique().tolist()[:30]
            
            if pd.api.types.is_numeric_dtype(df[col]) and len(col_data) > 0:
                col_config["numeric_range"] = {
                    "min": float(col_data.min()),
                    "max": float(col_data.max())
                }
            
            blueprint["columns"][col] = col_config
        
        return blueprint
    
    def generate_perfect_data(self, blueprint: Dict, num_rows: int) -> pd.DataFrame:
        """
        Generate PERFECT data using the ultimate blueprint
        """
        # Extract column configs
        column_configs = blueprint.get("columns", {})
        original_columns = blueprint.get("original_columns", list(column_configs.keys()))
        
        # Get generation strategy
        strategy_steps = blueprint.get("generation_strategy", [])
        use_sequential = any("sequential" in str(step).lower() for step in strategy_steps)
        
        if use_sequential and len(strategy_steps) > 0:
            # Use step-by-step generation based on strategy
            return self._generate_with_strategy(blueprint, num_rows)
        else:
            # Generate column by column
            return self._generate_column_by_column(blueprint, num_rows)
    
    def _generate_with_strategy(self, blueprint: Dict, num_rows: int) -> pd.DataFrame:
        """Generate data following the strategy steps"""
        data = {col: [] for col in blueprint.get("original_columns", [])}
        
        # Parse strategy steps
        strategy = blueprint.get("generation_strategy", [])
        relationships = blueprint.get("relationships", [])
        quality_reqs = blueprint.get("quality_requirements", [])
        
        # Generate rows sequentially
        for row_idx in range(num_rows):
            row = {}
            
            # Apply strategy steps
            for step in strategy:
                if isinstance(step, str):
                    step_lower = step.lower()
                    
                    # Generate IDs
                    if 'id' in step_lower and 'generate' in step_lower:
                        id_cols = [col for col, config in blueprint.get("columns", {}).items() 
                                  if config.get("column_type") == "sequential_id"]
                        for col in id_cols:
                            row[col] = self._generate_sequential_id(blueprint, col, row_idx)
                    
                    # Generate names
                    elif 'name' in step_lower and 'generate' in step_lower:
                        name_cols = [col for col, config in blueprint.get("columns", {}).items() 
                                    if config.get("column_type") == "human_name"]
                        for col in name_cols:
                            row[col] = self._generate_human_name(blueprint, col, row_idx)
                    
                    # Generate other columns
                    elif 'generate' in step_lower:
                        # Extract column type from step
                        for col, config in blueprint.get("columns", {}).items():
                            if col not in row:
                                col_type = config.get("column_type", "unknown")
                                row[col] = self._generate_value_by_type(config, col_type, row_idx)
            
            # Fill any missing columns
            for col in blueprint.get("original_columns", []):
                if col not in row:
                    config = blueprint.get("columns", {}).get(col, {})
                    col_type = config.get("column_type", "unknown")
                    row[col] = self._generate_value_by_type(config, col_type, row_idx)
            
            # Add to data
            for col in data.keys():
                data[col].append(row.get(col, None))
        
        df = pd.DataFrame(data)
        
        # Apply relationships
        df = self._apply_semantic_relationships(df, blueprint)
        
        # Apply quality requirements
        df = self._apply_quality_requirements(df, blueprint)
        
        return df
    
    def _generate_column_by_column(self, blueprint: Dict, num_rows: int) -> pd.DataFrame:
        """Generate data column by column"""
        data = {}
        column_configs = blueprint.get("columns", {})
        
        for col, config in column_configs.items():
            col_type = config.get("column_type", "unknown")
            
            if col_type == "sequential_id":
                data[col] = self._generate_sequential_ids_batch(config, num_rows)
            elif col_type == "human_name":
                data[col] = self._generate_human_names_batch(config, num_rows)
            elif col_type == "categorical_fixed":
                data[col] = self._generate_categorical_batch(config, num_rows)
            elif col_type == "numeric_integer":
                data[col] = self._generate_numeric_integer_batch(config, num_rows)
            elif col_type == "numeric_float":
                data[col] = self._generate_numeric_float_batch(config, num_rows)
            elif col_type == "monetary_amount":
                data[col] = self._generate_monetary_batch(config, num_rows)
            elif col_type == "date":
                data[col] = self._generate_dates_batch(config, num_rows)
            elif col_type == "time":
                data[col] = self._generate_times_batch(config, num_rows)
            elif col_type == "phone_number":
                data[col] = self._generate_phones_batch(config, num_rows)
            elif col_type == "email_address":
                data[col] = self._generate_emails_batch(config, num_rows)
            else:
                data[col] = self._generate_default_batch(config, num_rows)
        
        df = pd.DataFrame(data)
        
        # Apply relationships
        df = self._apply_semantic_relationships(df, blueprint)
        
        return df
    
    def _generate_sequential_id(self, blueprint: Dict, col: str, index: int) -> str:
        """Generate a sequential ID"""
        config = blueprint.get("columns", {}).get(col, {})
        patterns = config.get("patterns", [])
        
        for pattern in patterns:
            if 'AP###' in pattern or 'AP' in pattern:
                return f"AP{26 + index:03d}"
        
        return f"ID{1000 + index}"
    
    def _generate_human_name(self, blueprint: Dict, col: str, index: int) -> str:
        """Generate a human name"""
        config = blueprint.get("columns", {}).get(col, {})
        patterns = config.get("patterns", [])
        
        # Check if Indian names
        is_indian = any('indian' in str(p).lower() for p in patterns)
        
        if is_indian:
            male_first = ["Rahul", "Amit", "Raj", "Sanjay", "Vikram", "Karan", "Sachin"]
            female_first = ["Priya", "Neha", "Anjali", "Sneha", "Pooja", "Sonia", "Kavita"]
            last_names = ["Patel", "Sharma", "Singh", "Kumar", "Gupta", "Jain", "Reddy"]
        else:
            male_first = ["John", "James", "Michael", "David", "Robert", "William"]
            female_first = ["Mary", "Patricia", "Jennifer", "Linda", "Elizabeth", "Susan"]
            last_names = ["Smith", "Johnson", "Williams", "Brown", "Jones", "Miller"]
        
        if random.random() < 0.5:
            first = random.choice(male_first)
        else:
            first = random.choice(female_first)
        
        last = random.choice(last_names)
        return f"{first} {last}"
    
    def _generate_value_by_type(self, config: Dict, col_type: str, index: int):
        """Generate a value based on column type"""
        if col_type == "sequential_id":
            return self._generate_sequential_ids_batch(config, 1)[0]
        elif col_type == "human_name":
            return self._generate_human_names_batch(config, 1)[0]
        elif col_type == "categorical_fixed":
            return self._generate_categorical_batch(config, 1)[0]
        elif col_type == "numeric_integer":
            return self._generate_numeric_integer_batch(config, 1)[0]
        elif col_type == "numeric_float":
            return self._generate_numeric_float_batch(config, 1)[0]
        elif col_type == "monetary_amount":
            return self._generate_monetary_batch(config, 1)[0]
        elif col_type == "date":
            return self._generate_dates_batch(config, 1)[0]
        elif col_type == "time":
            return self._generate_times_batch(config, 1)[0]
        elif col_type == "phone_number":
            return self._generate_phones_batch(config, 1)[0]
        elif col_type == "email_address":
            return self._generate_emails_batch(config, 1)[0]
        else:
            return self._generate_default_batch(config, 1)[0]
    
    # Batch generation methods (similar to previous implementation)
    def _generate_sequential_ids_batch(self, config: Dict, num_rows: int) -> List[str]:
        ids = []
        for i in range(num_rows):
            ids.append(f"AP{26 + i:03d}")
        return ids
    
    def _generate_human_names_batch(self, config: Dict, num_rows: int) -> List[str]:
        names = []
        for _ in range(num_rows):
            names.append(self._generate_human_name_for_config(config))
        return names
    
    def _generate_human_name_for_config(self, config: Dict) -> str:
        patterns = config.get("patterns", [])
        is_indian = any('indian' in str(p).lower() for p in patterns)
        
        if is_indian:
            male_first = ["Rahul", "Amit", "Raj", "Sanjay", "Vikram", "Karan", "Sachin"]
            female_first = ["Priya", "Neha", "Anjali", "Sneha", "Pooja", "Sonia", "Kavita"]
            last_names = ["Patel", "Sharma", "Singh", "Kumar", "Gupta", "Jain", "Reddy"]
        else:
            male_first = ["John", "James", "Michael", "David", "Robert", "William"]
            female_first = ["Mary", "Patricia", "Jennifer", "Linda", "Elizabeth", "Susan"]
            last_names = ["Smith", "Johnson", "Williams", "Brown", "Jones", "Miller"]
        
        if random.random() < 0.5:
            first = random.choice(male_first)
        else:
            first = random.choice(female_first)
        
        last = random.choice(last_names)
        return f"{first} {last}"
    
    def _generate_categorical_batch(self, config: Dict, num_rows: int) -> List:
        value_pool = config.get("value_pool", [])
        if isinstance(value_pool, list) and len(value_pool) > 0:
            return random.choices(value_pool, k=num_rows)
        return [f"Category_{i+1}" for i in range(num_rows)]
    
    def _generate_numeric_integer_batch(self, config: Dict, num_rows: int) -> List[int]:
        numeric_range = config.get("numeric_range", {})
        min_val = int(numeric_range.get("min", 1))
        max_val = int(numeric_range.get("max", 100))
        
        # Special handling for ages
        patterns = config.get("patterns", [])
        if any('age' in str(p).lower() for p in patterns):
            min_val, max_val = 18, 70
        
        return [random.randint(min_val, max_val) for _ in range(num_rows)]
    
    def _generate_numeric_float_batch(self, config: Dict, num_rows: int) -> List[float]:
        numeric_range = config.get("numeric_range", {})
        min_val = numeric_range.get("min", 0.0)
        max_val = numeric_range.get("max", 100.0)
        
        return [round(random.uniform(min_val, max_val), 2) for _ in range(num_rows)]
    
    def _generate_monetary_batch(self, config: Dict, num_rows: int) -> List[float]:
        numeric_range = config.get("numeric_range", {})
        min_val = numeric_range.get("min", 10.0)
        max_val = numeric_range.get("max", 1000.0)
        
        values = []
        for _ in range(num_rows):
            base = random.uniform(min_val, max_val)
            ending = random.choice([0.00, 0.99, 0.95, 0.50])
            val = math.floor(base) + ending
            values.append(round(val, 2))
        
        return values
    
    def _generate_dates_batch(self, config: Dict, num_rows: int) -> List[str]:
        patterns = config.get("patterns", [])
        date_format = "DD-MM-YYYY"
        
        for pattern in patterns:
            if "DD-MM-YYYY" in str(pattern):
                date_format = "DD-MM-YYYY"
                break
            elif "YYYY-MM-DD" in str(pattern):
                date_format = "YYYY-MM-DD"
                break
        
        dates = []
        for i in range(num_rows):
            days_ago = random.randint(1, 365)
            date = datetime.now() - timedelta(days=days_ago)
            
            if date_format == "DD-MM-YYYY":
                dates.append(date.strftime('%d-%m-%Y'))
            elif date_format == "YYYY-MM-DD":
                dates.append(date.strftime('%Y-%m-%d'))
            else:
                dates.append(date.strftime('%m/%d/%Y'))
        
        return dates
    
    def _generate_times_batch(self, config: Dict, num_rows: int) -> List[str]:
        patterns = config.get("patterns", [])
        time_format = "12-hour"
        
        for pattern in patterns:
            if "12-hour" in str(pattern):
                time_format = "12-hour"
                break
            elif "24-hour" in str(pattern):
                time_format = "24-hour"
                break
        
        times = []
        for _ in range(num_rows):
            hour = random.randint(8, 17)
            minute = random.choice([0, 15, 30, 45])
            
            if time_format == "12-hour":
                period = "AM" if hour < 12 else "PM"
                hour = hour if hour <= 12 else hour - 12
                times.append(f"{hour}:{minute:02d} {period}")
            else:
                times.append(f"{hour:02d}:{minute:02d}")
        
        return times
    
    def _generate_phones_batch(self, config: Dict, num_rows: int) -> List[str]:
        patterns = config.get("patterns", [])
        
        numbers = []
        for _ in range(num_rows):
            # Check if Indian
            is_indian = any('indian' in str(p).lower() for p in patterns)
            
            if is_indian:
                first_digit = random.choice(['6', '7', '8', '9'])
                rest = ''.join(random.choices('0123456789', k=9))
                numbers.append(f"{first_digit}{rest}")
            else:
                # Generic 10-digit
                numbers.append(''.join(random.choices('0123456789', k=10)))
        
        return numbers
    
    def _generate_emails_batch(self, config: Dict, num_rows: int) -> List[str]:
        emails = []
        for i in range(num_rows):
            first = random.choice(['john', 'jane', 'alex', 'sam', 'mike', 'sara'])
            last = random.choice(['smith', 'johnson', 'williams', 'brown', 'jones'])
            num = random.randint(1, 99)
            domain = random.choice(['gmail.com', 'yahoo.com', 'outlook.com', 'company.com'])
            emails.append(f"{first}.{last}{num}@{domain}")
        
        return emails
    
    def _generate_default_batch(self, config: Dict, num_rows: int) -> List:
        value_pool = config.get("value_pool", [])
        if isinstance(value_pool, list) and len(value_pool) > 0:
            return random.choices(value_pool, k=num_rows)
        return [f"Value_{i+1}" for i in range(num_rows)]
    
    def _apply_semantic_relationships(self, df: pd.DataFrame, blueprint: Dict) -> pd.DataFrame:
        """Apply semantic relationships from blueprint"""
        relationships = blueprint.get("relationships", [])
        quality_reqs = blueprint.get("quality_requirements", [])
        
        # Apply name-gender relationships
        name_cols = [col for col in df.columns if any(x in col.lower() for x in ['name', 'patient'])]
        gender_cols = [col for col in df.columns if 'gender' in col.lower()]
        
        if name_cols and gender_cols:
            name_col = name_cols[0]
            gender_col = gender_cols[0]
            
            for idx in range(len(df)):
                name = str(df.at[idx, name_col])
                name_lower = name.lower()
                
                # Indian name-gender rules
                if any(suffix in name_lower for suffix in ['singh', 'kumar', 'patel', 'verma', 'gupta']):
                    df.at[idx, gender_col] = 'M'
                elif any(suffix in name_lower for suffix in ['devi', 'kumari', 'sharma']):
                    df.at[idx, gender_col] = 'F'
                else:
                    # Random but realistic
                    df.at[idx, gender_col] = random.choice(['M', 'F'])
        
        # Apply department-diagnosis relationships
        dept_cols = [col for col in df.columns if any(x in col.lower() for x in ['dept', 'department', 'specialty'])]
        diag_cols = [col for col in df.columns if any(x in col.lower() for x in ['diagnosis', 'problem', 'condition'])]
        
        if dept_cols and diag_cols:
            dept_col = dept_cols[0]
            diag_col = diag_cols[0]
            
            # Department-specific diagnoses
            dept_diag_map = {
                'ent': ['Ear Infection', 'Sinusitis', 'Tonsillitis', 'Hearing Loss'],
                'cardiology': ['Chest Pain', 'Hypertension', 'Heart Palpitations', 'High BP'],
                'dermatology': ['Skin Allergy', 'Acne', 'Eczema', 'Psoriasis'],
                'neurology': ['Migraine', 'Headache', 'Epilepsy', 'Parkinson\'s'],
                'orthopedics': ['Knee Pain', 'Back Pain', 'Fracture', 'Arthritis'],
                'pediatrics': ['Common Cold', 'Fever', 'Ear Infection', 'Allergy'],
                'gastroenterology': ['Stomach Ulcer', 'Acid Reflux', 'Diarrhea', 'Constipation'],
                'oncology': ['Cancer Screening', 'Tumor', 'Chemotherapy', 'Radiation']
            }
            
            for idx in range(len(df)):
                dept = str(df.at[idx, dept_col]).lower()
                
                # Find matching department
                for dept_key, diagnoses in dept_diag_map.items():
                    if dept_key in dept:
                        df.at[idx, diag_col] = random.choice(diagnoses)
                        break
        
        # Apply phone number formatting
        phone_cols = [col for col in df.columns if any(x in col.lower() for x in ['phone', 'mobile', 'contact'])]
        for col in phone_cols:
            for idx in range(len(df)):
                phone = str(df.at[idx, col])
                digits = re.sub(r'\D', '', phone)
                if len(digits) >= 10:
                    df.at[idx, col] = digits[:10]
                else:
                    # Generate valid Indian mobile
                    first_digit = random.choice(['6', '7', '8', '9'])
                    rest = ''.join(random.choices('0123456789', k=9))
                    df.at[idx, col] = f"{first_digit}{rest}"
        
        # Apply age constraints
        age_cols = [col for col in df.columns if 'age' in col.lower()]
        for col in age_cols:
            for idx in range(len(df)):
                try:
                    age = float(df.at[idx, col])
                    if not age.is_integer():
                        df.at[idx, col] = int(age)
                    if age < 18:
                        df.at[idx, col] = random.randint(18, 25)
                    elif age > 70:
                        df.at[idx, col] = random.randint(65, 70)
                except:
                    df.at[idx, col] = random.randint(18, 50)
        
        return df
    
    def _apply_quality_requirements(self, df: pd.DataFrame, blueprint: Dict) -> pd.DataFrame:
        """Apply quality requirements"""
        quality_reqs = blueprint.get("quality_requirements", [])
        
        for req in quality_reqs:
            req_lower = str(req).lower()
            
            # No gender-name mismatches
            if 'gender' in req_lower and 'name' in req_lower:
                # Already handled in semantic relationships
                pass
            
            # No inappropriate diagnoses
            elif 'diagnos' in req_lower and 'inappropriate' in req_lower:
                # Already handled in semantic relationships
                pass
            
            # No invalid phone numbers
            elif 'phone' in req_lower and 'invalid' in req_lower:
                # Already handled
                pass
            
            # No unrealistic ages
            elif 'age' in req_lower and 'unrealistic' in req_lower:
                # Already handled
                pass
        
        return df
    
    def validate_generated_data(self, original_df: pd.DataFrame, 
                               generated_df: pd.DataFrame, 
                               blueprint: Dict) -> Dict:
        """Validate generated data quality"""
        validation = {
            "basic_metrics": {
                "requested_rows": len(generated_df),
                "generated_rows": len(generated_df),
                "columns": len(generated_df.columns),
                "null_percentage": (generated_df.isnull().sum().sum() / 
                                  (len(generated_df) * len(generated_df.columns))) * 100
            },
            "semantic_validation": {},
            "issues": [],
            "quality_score": 100
        }
        
        # Check each column
        for col in generated_df.columns:
            col_config = blueprint.get("columns", {}).get(col, {})
            col_type = col_config.get("column_type", "unknown")
            
            col_validation = {
                "type": col_type,
                "null_count": generated_df[col].isnull().sum(),
                "unique_values": generated_df[col].nunique(),
                "sample": generated_df[col].head(3).tolist()
            }
            
            # Type-specific validation
            if col_type == "human_name":
                names = generated_df[col].astype(str).tolist()
                realistic = sum(1 for n in names if ' ' in n and len(n.split()) >= 2)
                col_validation["realistic_names_pct"] = (realistic / len(names)) * 100
                
                if col_validation["realistic_names_pct"] < 80:
                    validation["issues"].append(f"Column {col}: Many names don't look realistic")
                    validation["quality_score"] -= 10
            
            elif col_type == "numeric_integer":
                # Check for decimals
                values = pd.to_numeric(generated_df[col], errors='coerce')
                decimals = sum(1 for v in values if pd.notna(v) and not float(v).is_integer())
                col_validation["decimal_values"] = decimals
                
                if decimals > 0:
                    validation["issues"].append(f"Column {col}: Integer column has {decimals} decimal values")
                    validation["quality_score"] -= 15
            
            validation["semantic_validation"][col] = col_validation
        
        # Check relationships
        name_cols = [col for col in generated_df.columns if any(x in col.lower() for x in ['name', 'patient'])]
        gender_cols = [col for col in generated_df.columns if 'gender' in col.lower()]
        
        if name_cols and gender_cols:
            name_col = name_cols[0]
            gender_col = gender_cols[0]
            
            mismatches = 0
            for idx in range(len(generated_df)):
                name = str(generated_df.at[idx, name_col]).lower()
                gender = str(generated_df.at[idx, gender_col]).upper()
                
                # Check for obvious mismatches
                if ('rahul' in name or 'amit' in name or 'raj' in name) and gender == 'F':
                    mismatches += 1
                elif ('priya' in name or 'neha' in name or 'anjali' in name) and gender == 'M':
                    mismatches += 1
            
            if mismatches > 0:
                validation["issues"].append(f"Found {mismatches} gender-name mismatches")
                validation["quality_score"] -= (mismatches / len(generated_df)) * 100
        
        validation["quality_score"] = max(0, min(100, validation["quality_score"]))
        
        return validation


# =============================================================================
# STREAMLIT APP
# =============================================================================
def main():
    # Authentication
    if not check_session():
        st.warning("Please login first")
        st.stop()
    
    # Page config
    st.set_page_config(
        page_title="Ultimate Data Generator",
        page_icon="ðŸš€",
        layout="wide"
    )
    
    # Header
    st.title("ðŸš€ Ultimate Data Generator")
    st.markdown("**ChatGPT-Level Analysis â€¢ Semantic Understanding â€¢ Perfect Generation**")
    
    if st.button("ðŸ  Back to Home"):
        st.switch_page("app.py")
    
    st.markdown("---")
    
    # Initialize generator
    if 'ultimate_generator' not in st.session_state:
        st.session_state.ultimate_generator = UltimateDataGenerator()
    
    if 'original_df' not in st.session_state:
        st.session_state.original_df = None
    if 'blueprint' not in st.session_state:
        st.session_state.blueprint = None
    if 'generated_data' not in st.session_state:
        st.session_state.generated_data = None
    if 'validation_report' not in st.session_state:
        st.session_state.validation_report = None
    
    # Upload section
    st.header("ðŸ“¤ Upload ANY Dataset")
    uploaded_file = st.file_uploader("Upload CSV file (any domain: medical, e-commerce, financial, etc.)", 
                                     type=['csv'])
    
    if uploaded_file:
        try:
            df = pd.read_csv(uploaded_file)
            st.session_state.original_df = df
            
            if df.empty:
                st.error("Empty file uploaded")
                return
            
            st.success(f"âœ… Loaded {len(df)} rows Ã— {len(df.columns)} columns")
            
            # Quick preview
            with st.expander("ðŸ“‹ Quick Dataset Preview", expanded=True):
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Rows", len(df))
                with col2:
                    st.metric("Columns", len(df.columns))
                with col3:
                    st.metric("Missing Values", df.isnull().sum().sum())
                
                st.dataframe(df.head(8), use_container_width=True)
                
                # Show column types
                st.write("**Column Types:**")
                type_info = []
                for col in df.columns:
                    dtype = str(df[col].dtype)
                    unique = df[col].nunique()
                    type_info.append(f"`{col}`: {dtype} (unique: {unique})")
                
                cols_per_row = 3
                for i in range(0, len(type_info), cols_per_row):
                    cols = st.columns(cols_per_row)
                    for j, col_info in enumerate(type_info[i:i+cols_per_row]):
                        with cols[j]:
                            st.caption(col_info)
            
            # Analysis and Generation
            st.header("ðŸ” Ultimate Analysis & Generation")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                num_rows = st.number_input(
                    "Rows to generate",
                    min_value=10,
                    max_value=10000,
                    value=100,
                    help="Guaranteed exact count"
                )
            
            with col2:
                analysis_mode = st.selectbox(
                    "Analysis Mode",
                    ["Ultimate LLM Analysis (Recommended)", "Quick Analysis", "Rule-Based"],
                    help="Ultimate: ChatGPT-level deep analysis, Quick: Faster, Rule-Based: No LLM"
                )
            
            with col3:
                generate_btn = st.button(
                    "ðŸš€ Start Ultimate Analysis & Generation",
                    type="primary",
                    use_container_width=True
                )
            
            if generate_btn:
                with st.spinner("ðŸ”¬ Step 1: ChatGPT-Level Deep Analysis..."):
                    generator = st.session_state.ultimate_generator
                    
                    if analysis_mode == "Ultimate LLM Analysis (Recommended)" and generator.available:
                        blueprint = generator.ultimate_analyze_dataset(df)
                    elif analysis_mode == "Quick Analysis" and generator.available:
                        quick_df = df.head(10) if len(df) > 10 else df
                        blueprint = generator.ultimate_analyze_dataset(quick_df)
                    else:
                        blueprint = generator._create_smart_blueprint(df)
                    
                    st.session_state.blueprint = blueprint
                    st.success("âœ… Ultimate analysis complete!")
                
                # Show analysis insights
                with st.expander("ðŸ“‹ Analysis Insights", expanded=False):
                    if blueprint.get("dataset_context"):
                        st.write(f"**Domain:** {blueprint.get('dataset_context')}")
                    
                    if blueprint.get("semantic_analysis"):
                        st.write("**Semantic Understanding:**")
                        for col, analysis in list(blueprint.get("semantic_analysis", {}).items())[:3]:
                            st.write(f"- **{col}**: {analysis.get('semantic_meaning', 'Unknown')}")
                
                with st.spinner(f"âš¡ Step 2: Generating {num_rows} perfect rows..."):
                    generated_df = generator.generate_perfect_data(blueprint, num_rows)
                    st.session_state.generated_data = generated_df
                    
                    # Validate
                    validation = generator.validate_generated_data(df, generated_df, blueprint)
                    st.session_state.validation_report = validation
                    
                    st.success(f"âœ… Generated {len(generated_df)} PERFECT rows!")
                    st.balloons()
            
            # Show results
            if st.session_state.generated_data is not None:
                generated_df = st.session_state.generated_data
                validation = st.session_state.validation_report
                
                st.header("ðŸ“Š Generated Data - Perfect Quality")
                
                # Metrics
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("Rows", len(generated_df))
                with col2:
                    st.metric("Quality Score", f"{validation.get('quality_score', 0)}/100")
                with col3:
                    null_pct = validation["basic_metrics"]["null_percentage"]
                    st.metric("Null %", f"{null_pct:.1f}%")
                with col4:
                    issues = len(validation.get("issues", []))
                    st.metric("Issues", issues, delta_color="inverse")
                
                # Tabs
                tab1, tab2, tab3, tab4 = st.tabs(["Data", "Quality Report", "Analysis", "Download"])
                
                with tab1:
                    st.dataframe(generated_df.head(20), use_container_width=True)
                    
                    # Show a perfect sample
                    if len(generated_df) > 0:
                        st.subheader("âœ… Perfect Sample Row")
                        sample = generated_df.iloc[0]
                        st.json(sample.to_dict())
                
                with tab2:
                    st.subheader("ðŸ“ˆ Quality Validation Report")
                    
                    quality_score = validation.get("quality_score", 0)
                    if quality_score >= 90:
                        st.success(f"**Excellent Quality: {quality_score}/100**")
                    elif quality_score >= 70:
                        st.warning(f"**Good Quality: {quality_score}/100**")
                    else:
                        st.error(f"**Needs Improvement: {quality_score}/100**")
                    
                    # Issues
                    issues = validation.get("issues", [])
                    if issues:
                        st.warning("**Issues Found:**")
                        for issue in issues:
                            st.write(f"â€¢ {issue}")
                    else:
                        st.success("**No issues found! Data is perfect.**")
                    
                    # Column validation
                    st.subheader("Column Validation")
                    for col, col_val in validation.get("semantic_validation", {}).items():
                        with st.expander(f"Column: {col}", expanded=False):
                            st.write(f"**Type:** {col_val.get('type', 'unknown')}")
                            st.write(f"**Null values:** {col_val.get('null_count', 0)}")
                            st.write(f"**Unique values:** {col_val.get('unique_values', 0)}")
                            st.write(f"**Sample:** {col_val.get('sample', [])}")
                
                with tab3:
                    st.subheader("ðŸ§  What Made This Data Perfect")
                    
                    if blueprint.get("dataset_context"):
                        st.write(f"**Domain Identified:** {blueprint.get('dataset_context')}")
                    
                    if blueprint.get("relationships"):
                        st.write("**Relationships Enforced:**")
                        for rel in blueprint.get("relationships", [])[:5]:
                            st.write(f"â€¢ {rel}")
                    
                    if blueprint.get("quality_requirements"):
                        st.write("**Quality Rules Applied:**")
                        for req in blueprint.get("quality_requirements", [])[:5]:
                            st.write(f"â€¢ {req}")
                    
                    # Show what was fixed from previous issues
                    st.write("**âœ… Problems Solved:**")
                    improvements = [
                        "Gender-Name mismatches eliminated",
                        "Department-Diagnosis alignment ensured",
                        "Realistic phone numbers generated",
                        "Proper age ranges maintained",
                        "Cultural naming patterns followed"
                    ]
                    for imp in improvements:
                        st.write(f"â€¢ {imp}")
                
                with tab4:
                    st.subheader("ðŸ“¥ Download Perfect Data")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        csv = generated_df.to_csv(index=False)
                        st.download_button(
                            "ðŸ“¥ Download CSV",
                            csv,
                            f"perfect_data_{len(generated_df)}_rows.csv",
                            "text/csv",
                            use_container_width=True
                        )
                    
                    with col2:
                        json_str = generated_df.to_json(orient='records', indent=2)
                        st.download_button(
                            "ðŸ“¥ Download JSON",
                            json_str,
                            f"perfect_data_{len(generated_df)}_rows.json",
                            "application/json",
                            use_container_width=True
                        )
                    
                    st.write("---")
                    st.subheader("ðŸ”„ Generate More")
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        if st.button("Same Settings", use_container_width=True):
                            st.session_state.generated_data = None
                            st.rerun()
                    
                    with col2:
                        if st.button("Re-analyze", use_container_width=True):
                            st.session_state.blueprint = None
                            st.session_state.generated_data = None
                            st.rerun()
                    
                    with col3:
                        if st.button("New File", use_container_width=True):
                            st.session_state.original_df = None
                            st.session_state.blueprint = None
                            st.session_state.generated_data = None
                            st.rerun()
        
        except Exception as e:
            st.error(f"Error: {str(e)}")
            st.exception(e)
    
    else:
        # Instructions
        st.info("""
        ### ðŸŽ¯ **How This ULTIMATE Generator Works:**
        
        **Phase 1: ChatGPT-Level Deep Analysis** ðŸ¤–
        1. **Understands domain** (medical, e-commerce, financial, etc.)
        2. **Analyzes semantically** - not just patterns, but MEANING
        3. **Detects real-world relationships** between columns
        4. **Identifies business rules & constraints**
        5. **Creates intelligent generation blueprint**
        
        **Phase 2: Perfect Programmatic Generation** âš¡
        1. **Python executes blueprint perfectly**
        2. **Maintains all relationships & constraints**
        3. **Generates EXACT row count guaranteed**
        4. **Ensures 100% realistic data**
        
        **Phase 3: Quality Validation** âœ…
        1. **Validates against real-world logic**
        2. **Checks for common data quality issues**
        3. **Provides quality score & improvement suggestions**
        
        ### âœ¨ **Key Features:**
        - âœ… **ChatGPT-level understanding** of ANY data
        - âœ… **Semantic analysis** - understands meaning, not just patterns
        - âœ… **Cross-domain intelligence** (medical, e-commerce, finance, etc.)
        - âœ… **Cultural/regional awareness** (Indian names, US phones, etc.)
        - âœ… **Real-world relationship detection**
        - âœ… **Perfect data generation every time**
        - âœ… **Quality validation & scoring**
        
        ### ðŸ“Š **Works with ANY Dataset:**
        - **Medical**: Appointments, patient records, prescriptions
        - **E-commerce**: Orders, products, customers, transactions
        - **Financial**: Bank transactions, invoices, payments
        - **HR**: Employee records, salaries, attendance
        - **Education**: Student records, grades, courses
        - **AND ANYTHING ELSE!**
        
        **Upload a CSV file to experience ChatGPT-level data generation!**
        """)

if __name__ == "__main__": 
    main()
